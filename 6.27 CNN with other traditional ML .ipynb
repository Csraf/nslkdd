{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('datasets/traindata.csv',header=None)\n",
    "Y = pd.read_csv('datasets/trainlabel_onehot.csv',header=None)\n",
    "testdata = pd.read_csv('datasets/testdata.csv',header=None)\n",
    "testlabel = pd.read_csv('datasets/testlabel_onehot.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huayanpei/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/huayanpei/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/huayanpei/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "x = ss.fit_transform(X)\n",
    "testdata = ss.transform(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cnn = np.reshape(x, (x.shape[0],x.shape[1],1))\n",
    "testdata_cnn = np.reshape(testdata, (testdata.shape[0], testdata.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(256,28, padding=\"same\", activation='relu', input_shape=(28,1)))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "#model.add(Conv1D(128, 28, padding=\"same\",activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125973 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "125973/125973 [==============================] - 16s 130us/step - loss: 0.0709 - acc: 0.9778 - val_loss: 1.7029 - val_acc: 0.7411\n",
      "Epoch 2/10\n",
      "125973/125973 [==============================] - 16s 127us/step - loss: 0.0446 - acc: 0.9850 - val_loss: 2.1514 - val_acc: 0.7465\n",
      "Epoch 3/10\n",
      "125973/125973 [==============================] - 16s 129us/step - loss: 0.0392 - acc: 0.9877 - val_loss: 1.9499 - val_acc: 0.7793\n",
      "Epoch 4/10\n",
      "125973/125973 [==============================] - 16s 129us/step - loss: 0.0364 - acc: 0.9887 - val_loss: 2.2416 - val_acc: 0.7561\n",
      "Epoch 5/10\n",
      "125973/125973 [==============================] - 16s 128us/step - loss: 0.0331 - acc: 0.9892 - val_loss: 2.1483 - val_acc: 0.7762\n",
      "Epoch 6/10\n",
      "125973/125973 [==============================] - 16s 130us/step - loss: 0.0320 - acc: 0.9902 - val_loss: 2.0528 - val_acc: 0.7709\n",
      "Epoch 7/10\n",
      "125973/125973 [==============================] - 16s 131us/step - loss: 0.0301 - acc: 0.9905 - val_loss: 2.3217 - val_acc: 0.7530\n",
      "Epoch 8/10\n",
      "125973/125973 [==============================] - 16s 130us/step - loss: 0.0289 - acc: 0.9906 - val_loss: 2.1365 - val_acc: 0.7744\n",
      "Epoch 9/10\n",
      "125973/125973 [==============================] - 17s 132us/step - loss: 0.0275 - acc: 0.9913 - val_loss: 2.2617 - val_acc: 0.7646\n",
      "Epoch 10/10\n",
      "125973/125973 [==============================] - 17s 135us/step - loss: 0.0280 - acc: 0.9913 - val_loss: 2.3656 - val_acc: 0.7878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3a297588>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_cnn, Y,epochs=10, callbacks=callbacks,validation_data=(testdata_cnn,testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = model.predict(testdata_cnn)\n",
    "for i in range(len(X_predict)):\n",
    "    max_value = max(X_predict[i])\n",
    "    for j in range(len(X_predict[i])):\n",
    "        if max_value == X_predict[i][j]:\n",
    "            X_predict[i][j] = 1\n",
    "        else:\n",
    "            X_predict[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     13482\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.18      0.94      0.31       533\n",
      "           3       0.72      0.82      0.76      2139\n",
      "           4       0.81      0.95      0.88      6390\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     22544\n",
      "   macro avg       0.54      0.68      0.55     22544\n",
      "weighted avg       0.89      0.79      0.82     22544\n",
      " samples avg       0.79      0.79      0.79     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(X_predict, testlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Darpa Dataset as validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import every week's darpa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Week1 = pd.read_csv('datasets/darpa_5/week1.csv',header=None)\n",
    "Week1_label = pd.read_csv('datasets/darpa_5/week1_label.csv',header=None)\n",
    "Week2 = pd.read_csv('datasets/darpa_5/week2.csv',header=None)\n",
    "Week2_label = pd.read_csv('datasets/darpa_5/week2_label.csv',header=None)\n",
    "Week3 = pd.read_csv('datasets/darpa_5/week3.csv',header=None)\n",
    "Week3_label = pd.read_csv('datasets/darpa_5/week3_label.csv',header=None)\n",
    "Week4 = pd.read_csv('datasets/darpa_5/week4.csv',header=None)\n",
    "Week4_label = pd.read_csv('datasets/darpa_5/week4_label.csv',header=None)\n",
    "Week5 = pd.read_csv('datasets/darpa_5/week5.csv',header=None)\n",
    "Week5_label = pd.read_csv('datasets/darpa_5/week5_label.csv',header=None)\n",
    "Week6 = pd.read_csv('datasets/darpa_5/week6.csv',header=None)\n",
    "Week6_label = pd.read_csv('datasets/darpa_5/week6_label.csv',header=None)\n",
    "Week7 = pd.read_csv('datasets/darpa_5/week7.csv',header=None)\n",
    "Week7_label = pd.read_csv('datasets/darpa_5/week7_label.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huayanpei/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Xall = pd.concat([Week1,Week2,Week3, Week4, Week5, Week6, Week7], axis=0, ignore_index=True)\n",
    "Yall = pd.concat([Week1_label,Week2_label,Week3_label, Week4_label, Week5_label, Week6_label, Week7_label], axis=0, ignore_index=True)\n",
    "\n",
    "X_all = ss.transform(Xall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(n_values=5)\n",
    "def onehot(self):\n",
    "    return encoder.fit_transform(self.values.reshape(-1,1)).toarray()\n",
    "\n",
    "Yall_oh = onehot(Yall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "darpadata_cnn = np.reshape(X_all, (X_all.shape[0], X_all.shape[1], 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN use Darpa dataset , accuracy 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnndarpa = model.predict(darpadata_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cnndarpa)):\n",
    "    max_value = max(cnndarpa[i])\n",
    "    for j in range(len(cnndarpa[i])):\n",
    "        if max_value == cnndarpa[i][j]:\n",
    "            cnndarpa[i][j] = 1\n",
    "        else:\n",
    "            cnndarpa[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915772050698108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huayanpei/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92    920052\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.11      0.00      0.00      9636\n",
      "           3       0.17      0.12      0.14     68273\n",
      "           4       0.93      0.98      0.96   1083379\n",
      "\n",
      "   micro avg       0.92      0.92      0.92   2081340\n",
      "   macro avg       0.43      0.40      0.40   2081340\n",
      "weighted avg       0.90      0.92      0.91   2081340\n",
      " samples avg       0.92      0.92      0.92   2081340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(cnndarpa, Yall_oh))\n",
    "print(classification_report(cnndarpa, Yall_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
